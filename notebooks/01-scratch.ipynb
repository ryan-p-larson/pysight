{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Work -- New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.py\n",
    "\n",
    "import cv2, dlib\n",
    "from pysight.utils import drawing_utils as drawing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DLIB_LAND = dlib.shape_predictor(model_paths[0])\n",
    "HAAR_EYES = cv2.CascadeClassifier(model_paths[1])\n",
    "HAAR_FACE = cv2.CascadeClassifier(model_paths[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Utils*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     5,
     8,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "def process_grayscale(img):\n",
    "    \"\"\" Converts the colorspace of an Image as an NDArray. \"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "def process_equalize(img):\n",
    "    return cv2.equalizeHist(img)\n",
    "\n",
    "def process_resize(img, size, interpolation):\n",
    "    pass\n",
    "\n",
    "def process_crop(img, bbox):\n",
    "    \"\"\" Returns the specified bbox of an image. \"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    return img[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "## Conversions\n",
    "def convert_cv_point_2_dlib(point):\n",
    "    \"\"\"Helper function to convert an OpenCV point to Dlib format (dlib.point).\"\"\"\n",
    "    return dlib.point(point[0], point[1])\n",
    "\n",
    "\n",
    "def convert_dlib_point_2_cv(point):\n",
    "    \"\"\"Helper function to convert a Dlib point to OpenCV format (tuple).\"\"\"\n",
    "    return np.float32((point.x, point.y))\n",
    "\n",
    "\n",
    "def convert_dlib_shape_2_cv(shape):\n",
    "    \"\"\"Helper function that converts an active appearance model to opencv.\"\"\"\n",
    "    return [convert_dlib_point_2_cv(p) for p in shape.parts()]\n",
    "\n",
    "def convert_cv_bbox_2_dlib(bbox):\n",
    "    \"\"\" Returns a left, top, right, bottom DLib Rectangle. \"\"\"\n",
    "    return dlib.rectangle(int(bbox[0]),\n",
    "                          int(bbox[1]),\n",
    "                          int(bbox[0]) + int(bbox[2]),\n",
    "                          int(bbox[1]) + int(bbox[3]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################\n",
    "## Landmark Extractions\n",
    "\n",
    "# extract eye crop\n",
    "# params(img, landmarks, l/r)\n",
    "# eye_coords = [landmarks.part(i) for i in l]...\n",
    "# get bbox from eye_coords\n",
    "# return cropped image from img\n",
    "\n",
    "def extract_pose_points(landmarks):\n",
    "    \"\"\" Extracts face pose points from Dlib landmarks. \"\"\"\n",
    "    # 0-indexed DLib landmarks for facial pose: nose, chin, leye, reye, lmouth, rmouth\n",
    "    pose_points = [30, 8, 36, 45, 48, 54]\n",
    "    return [convert_dlib_point_2_cv(landmarks.part(idx)) for idx in pose_points]\n",
    "#     return convert_dlib_shape_2_cv(landmarks)\n",
    "\n",
    "# Left Eye: 36-41\n",
    "# Right eye: 42-47\n",
    "\n",
    "\n",
    "\n",
    "##########################################\n",
    "## To-Do\n",
    "\n",
    "# cropping\n",
    "\n",
    "# camera calibration/config\n",
    "# ratio\n",
    "\n",
    "# Dlib Landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Camera Stuff*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_camera_matrix(img):\n",
    "    #Obtaining the CAM dimension\n",
    "    cam_w = img.shape[0] # int(video_capture.get(3))\n",
    "    cam_h = img.shape[1] # int(video_capture.get(4))\n",
    "\n",
    "    # Define the camera matrix.\n",
    "    # To have better result it is necessary to find the focal\n",
    "    # lenght of the camera. fx/fy are the focal lengths (in pixels) \n",
    "    # and cx/cy are the optical centres. These values can be obtained \n",
    "    # roughly by approximation, for example in a 640x480 camera:\n",
    "    # cx = 640/2 = 320\n",
    "    # cy = 480/2 = 240\n",
    "    # fx = fy = cx/tan(60/2 * pi / 180) = 554.26\n",
    "    c_x = cam_w / 2\n",
    "    c_y = cam_h / 2\n",
    "    f_x = c_x / np.tan(60 / 2 * np.pi / 180)\n",
    "    f_y = f_x\n",
    "\n",
    "    #Estimated camera matrix values.\n",
    "    camera_matrix = np.float32([[f_x, 0.0, c_x],\n",
    "                                   [0.0, f_y, c_y], \n",
    "                                   [0.0, 0.0, 1.0] ])\n",
    "    return camera_matrix\n",
    "\n",
    "def get_distortion_matrix():\n",
    "    return np.zeros((5, 1), np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     1,
     7,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# Cameria calibration from utils/calibration.py\n",
    "cam_mat = np.array([[789.90059985,   0.        , 590.13037513],\n",
    "       [  0.        , 829.21211186, -58.1549458 ],\n",
    "       [  0.        ,   0.        ,   1.        ]])\n",
    "\n",
    "cam_dist = np.array([[-0.15982226,  0.28575061,  0.00784566, -0.05211988, -0.13169634]])\n",
    "\n",
    "cam_rvecs = np.array([[ 0.18616054],\n",
    "       [ 0.19607688],\n",
    "       [-0.02474464]])\n",
    "\n",
    "cam_tvecs = np.array([[-0.43361849],\n",
    "       [ 5.77655689],\n",
    "       [13.38584679]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/user/develop/working/computervision/pysight/tests/test_imgs/scrot2.png',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/webcam-single-00.jpg',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/scrot3.png',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/eye-control.jpg',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/eye-double.jpg',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/webcam-bad-eyes.jpg',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/eye-crop-0.jpg',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/scrot.png',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/webcam-single-01.png',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/webcam-multiple.jpg',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/webcam-bad-face.jpg',\n",
       " '/home/user/develop/working/computervision/pysight/tests/test_imgs/eye-single.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## work starts here  #####################################################################\n",
    "test_img_path = img_paths[0]\n",
    "test_img = cv2.imread(test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "test_img_proc = process_grayscale(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Facial Bounding box*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "def detect_faces(img_gray):\n",
    "    \"\"\" Returns the first bounding box of a face found in grayscale image. \"\"\"\n",
    "    detected_faces = HAAR_FACE.detectMultiScale(img_gray, 1.3, 5)\n",
    "    return detected_faces[0] if len(detected_faces) > 0 else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-01cd2cd7529a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bbox_face = detect_faces(test_img_proc)\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/pysight/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2357\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2358\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2359\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/user/.virtualenvs/pysight/lib/python3.7/site-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pysight/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pysight/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1160\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         \u001b[0mall_runs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1163\u001b[0m         \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0mworst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_runs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnumber\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.7/timeit.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(self, repeat, number)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pysight/lib/python3.7/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-75afba52e098>\u001b[0m in \u001b[0;36mdetect_faces\u001b[0;34m(img_gray)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\" Returns the first bounding box of a face found in grayscale image. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdetected_faces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHAAR_FACE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetected_faces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetected_faces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "bbox_face = detect_faces(test_img_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_face = detect_faces(test_img_proc)\n",
    "bbox_face_dlib = convert_cv_bbox_2_dlib(bbox_face)\n",
    "out_img = drawing.draw_rectangle(test_img_proc.copy(), bbox_face[0], bbox_face[1], bbox_face[2], bbox_face[3])\n",
    "showarray(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landmarks\n",
    "\n",
    "![DLIB](https://miro.medium.com/max/700/1*mArsPXT2PB19dF4sPR-VSA.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmarks\n",
    "face_landmarks_dlib = DLIB_LAND(test_img_proc, bbox_face_dlib)\n",
    "face_landmarks_cv2 = convert_dlib_shape_2_cv(face_landmarks_dlib)\n",
    "\n",
    "# visualize it\n",
    "out_img = test_img_proc.copy()\n",
    "for x, y in extract_pose_points(face_landmarks_dlib):\n",
    "    out_img = drawing.draw_circle(out_img, x, y, 3)\n",
    "showarray(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmark model\n",
    "pp = [30, 8, 45, 36, 54, 48]\n",
    "\n",
    "# Anthropogenic model\n",
    "P3D_NOSE      = np.float32([21.1, 0.0, -48.0])  #30\n",
    "P3D_MENTON    = np.float32([0.0,  0.0, -122.7])  #8\n",
    "P3D_LEFT_EYE  = np.float32([-20.0, 65.5,-5.0])  #45\n",
    "P3D_RIGHT_EYE = np.float32([-20.0, -65.5,-5.0]) #36\n",
    "P3D_LIP_LEFT  = np.float32([-20.0, 65.5,-5.0])  #54\n",
    "P3D_LIP_RIGHT = np.float32([-20.0, 65.5,-5.0])  #48\n",
    "\n",
    "\n",
    "landmarks_3D = np.float32([\n",
    "    P3D_NOSE,\n",
    "    P3D_MENTON,\n",
    "    P3D_LEFT_EYE,\n",
    "    P3D_RIGHT_EYE,\n",
    "    P3D_LIP_LEFT,\n",
    "    P3D_LIP_RIGHT\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(img, matrix, distortion, cls, bbox):\n",
    "    \"\"\"extract_landmarks(frame, cam_matrix, dlib.shape_predictor(\"...\"), face_bbox)\n",
    "    \"\"\"\n",
    "    # convert bbox to dlib\n",
    "    dbox = convert_cv_bbox_2_dlib(bbox)\n",
    "    \n",
    "    # detect landmarks\n",
    "    landmarks = cls(img, dbox)\n",
    "    \n",
    "    # Convert landmarks to NumPy format to solvePNP\n",
    "    l2D = np.float32([convert_dlib_point_2_cv(landmarks.part(idx)) for idx in pp])\n",
    "    \n",
    "    # From: https://stackoverflow.com/questions/44042323/opencv-error-assertion-failed-in-undistort-cpp-at-line-293\n",
    "    # np.ascontiguousarray(D[:,:2]).reshape((N,1,2))\n",
    "    landmarks_2D = np.ascontiguousarray(l2D[:,:2]).reshape((l2D.shape[0],1,2))\n",
    "    \n",
    "    ok, rvec, tvec = cv2.solvePnP(\n",
    "            landmarks_3D, landmarks_2D, matrix, distortion, flags=cv2.SOLVEPNP_EPNP)\n",
    "    if ok:\n",
    "        return (landmarks_2D, rvec, tvec)\n",
    "    else:\n",
    "        raise Exception(\"solvePNP failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "extract_landmarks(test_img_proc,\n",
    "                  cam_mat, #get_camera_matrix(test_img_proc),\n",
    "                  cam_dist, #get_distortion_matrix(),\n",
    "                  DLIB_LAND,\n",
    "                  bbox_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = test_img.copy()\n",
    "\n",
    "matrix_test = get_camera_matrix(out_img)\n",
    "distortion_test = get_distortion_matrix()\n",
    "\n",
    "landmarks_2D_test, rvec_test, tvec_test = extract_landmarks(\n",
    "        out_img, matrix_test, distortion_test, DLIB_LAND, bbox_face)\n",
    "\n",
    "#Now we project the 3D points into the image plane\n",
    "#Creating a 3-axis to be used as reference in the image.\n",
    "axis = np.float32([[50, 0, 0], [0, 50, 0], [0, 0, 50]])\n",
    "imgpts, jac = cv2.projectPoints(axis, rvec_test, tvec_test, matrix_test, distortion_test)\n",
    "\n",
    "for x, y in extract_pose_points(face_landmarks_dlib):\n",
    "    out_img = drawing.draw_circle(out_img, x, y, 3)\n",
    "\n",
    "corner = tuple(landmarks_2D_test[0].ravel())\n",
    "out_img = cv2.line(out_img, corner, tuple(imgpts[0].ravel()), (255, 0, 0), 5) # RED\n",
    "out_img = cv2.line(out_img, corner, tuple(imgpts[1].ravel()), (0, 255, 0), 5) # GREEN\n",
    "out_img = cv2.line(out_img, corner, tuple(imgpts[2].ravel()), (0, 0, 255), 5) # BLUE\n",
    "\n",
    "showarray(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "*Eyes* \n",
    "    \n",
    "    HaarCaascade | Extract BBox from Landmarks | Find Pupil Centers | Find Gaze Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://github.com/mpatacchiola/deepgaze/blob/master/examples/ex_dlib_pnp_head_pose_estimation_video.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "##\n",
    "# Massimiliano Patacchiola, Plymouth University 2016\n",
    "#\n",
    "# This is an example of head pose estimation with solvePnP and dlib face detector.\n",
    "# It uses the dlib library and openCV.\n",
    "# To use this example you have to provide an input video file\n",
    "# and an output path:\n",
    "# python ex_pnp_pose_estimation_video.py /home/video.mpg ./output.avi\n",
    "#\n",
    "\n",
    "import numpy\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from deepgaze.face_landmark_detection import faceLandmarkDetection\n",
    "\n",
    "#For the frontal face detector\n",
    "import dlib\n",
    "\n",
    "#Antropometric constant values of the human head. \n",
    "#Found on wikipedia and on:\n",
    "# \"Head-and-Face Anthropometric Survey of U.S. Respirator Users\"\n",
    "#\n",
    "#X-Y-Z with X pointing forward and Y on the left.\n",
    "#The X-Y-Z coordinates used are like the standard\n",
    "# coordinates of ROS (robotic operative system)\n",
    "P3D_RIGHT_SIDE = numpy.float32([-100.0, -77.5, -5.0]) #0\n",
    "P3D_GONION_RIGHT = numpy.float32([-110.0, -77.5, -85.0]) #4\n",
    "P3D_MENTON = numpy.float32([0.0, 0.0, -122.7]) #8\n",
    "P3D_GONION_LEFT = numpy.float32([-110.0, 77.5, -85.0]) #12\n",
    "P3D_LEFT_SIDE = numpy.float32([-100.0, 77.5, -5.0]) #16\n",
    "P3D_FRONTAL_BREADTH_RIGHT = numpy.float32([-20.0, -56.1, 10.0]) #17\n",
    "P3D_FRONTAL_BREADTH_LEFT = numpy.float32([-20.0, 56.1, 10.0]) #26\n",
    "P3D_SELLION = numpy.float32([0.0, 0.0, 0.0]) #27\n",
    "P3D_NOSE = numpy.float32([21.1, 0.0, -48.0]) #30\n",
    "P3D_SUB_NOSE = numpy.float32([5.0, 0.0, -52.0]) #33\n",
    "P3D_RIGHT_EYE = numpy.float32([-20.0, -65.5,-5.0]) #36\n",
    "P3D_RIGHT_TEAR = numpy.float32([-10.0, -40.5,-5.0]) #39\n",
    "P3D_LEFT_TEAR = numpy.float32([-10.0, 40.5,-5.0]) #42\n",
    "P3D_LEFT_EYE = numpy.float32([-20.0, 65.5,-5.0]) #45\n",
    "#P3D_LIP_RIGHT = numpy.float32([-20.0, 65.5,-5.0]) #48\n",
    "#P3D_LIP_LEFT = numpy.float32([-20.0, 65.5,-5.0]) #54\n",
    "P3D_STOMION = numpy.float32([10.0, 0.0, -75.0]) #62\n",
    "\n",
    "#The points to track\n",
    "#These points are the ones used by PnP\n",
    "# to estimate the 3D pose of the face\n",
    "TRACKED_POINTS = (0, 4, 8, 12, 16, 17, 26, 27, 30, 33, 36, 39, 42, 45, 62)\n",
    "ALL_POINTS = list(range(0,68)) #Used for debug only\n",
    "\n",
    "def main():\n",
    "\n",
    "    #Check if some argumentshave been passed\n",
    "    #pass the path of a video\n",
    "    if(len(sys.argv) > 2):\n",
    "        file_path = sys.argv[1]\n",
    "        if(os.path.isfile(file_path)==False): \n",
    "            print(\"ex_pnp_head_pose_estimation: the file specified does not exist.\")\n",
    "            return\n",
    "        else:\n",
    "            #Open the video file\n",
    "            video_capture = cv2.VideoCapture(file_path)\n",
    "            if(video_capture.isOpened() == True): print(\"ex_pnp_head_pose_estimation: the video source has been opened correctly...\")\n",
    "            # Define the codec and create VideoWriter object\n",
    "            #fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            output_path = sys.argv[2]\n",
    "            fourcc = cv2.cv.CV_FOURCC(*'XVID')\n",
    "            out = cv2.VideoWriter(output_path, fourcc, 20.0, (1280,720))\n",
    "    else:\n",
    "        print(\"You have to pass as argument the path to a video file and the path to the output file to produce, for example: \\n python ex_pnp_pose_estimation_video.py /home/video.mpg ./output.avi\")\n",
    "        return\n",
    "\n",
    "    #Create the main window and move it\n",
    "    cv2.namedWindow('Video')\n",
    "    cv2.moveWindow('Video', 20, 20)\n",
    "\n",
    "    #Obtaining the CAM dimension\n",
    "    cam_w = int(video_capture.get(3))\n",
    "    cam_h = int(video_capture.get(4))\n",
    "\n",
    "    #Defining the camera matrix.\n",
    "    #To have better result it is necessary to find the focal\n",
    "    # lenght of the camera. fx/fy are the focal lengths (in pixels) \n",
    "    # and cx/cy are the optical centres. These values can be obtained \n",
    "    # roughly by approximation, for example in a 640x480 camera:\n",
    "    # cx = 640/2 = 320\n",
    "    # cy = 480/2 = 240\n",
    "    # fx = fy = cx/tan(60/2 * pi / 180) = 554.26\n",
    "    c_x = cam_w / 2\n",
    "    c_y = cam_h / 2\n",
    "    f_x = c_x / numpy.tan(60/2 * numpy.pi / 180)\n",
    "    f_y = f_x\n",
    "\n",
    "    #Estimated camera matrix values.\n",
    "    camera_matrix = numpy.float32([[f_x, 0.0, c_x],\n",
    "                                   [0.0, f_y, c_y], \n",
    "                                   [0.0, 0.0, 1.0] ])\n",
    "\n",
    "    print(\"Estimated camera matrix: \\n\" + str(camera_matrix) + \"\\n\")\n",
    "\n",
    "    #These are the camera matrix values estimated on my webcam with\n",
    "    # the calibration code (see: src/calibration):\n",
    "    #camera_matrix = numpy.float32([[602.10618226,          0.0, 320.27333589],\n",
    "                                   #[         0.0, 603.55869786,  229.7537026], \n",
    "                                   #[         0.0,          0.0,          1.0] ])\n",
    "\n",
    "    #Distortion coefficients\n",
    "    camera_distortion = numpy.float32([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    #Distortion coefficients estimated by calibration\n",
    "    #camera_distortion = numpy.float32([ 0.06232237, -0.41559805,  0.00125389, -0.00402566,  0.04879263])\n",
    "\n",
    "\n",
    "    #This matrix contains the 3D points of the\n",
    "    # 11 landmarks we want to find. It has been\n",
    "    # obtained from antrophometric measurement\n",
    "    # on the human head.\n",
    "    landmarks_3D = numpy.float32([P3D_RIGHT_SIDE,\n",
    "                                  P3D_GONION_RIGHT,\n",
    "                                  P3D_MENTON,\n",
    "                                  P3D_GONION_LEFT,\n",
    "                                  P3D_LEFT_SIDE,\n",
    "                                  P3D_FRONTAL_BREADTH_RIGHT,\n",
    "                                  P3D_FRONTAL_BREADTH_LEFT,\n",
    "                                  P3D_SELLION,\n",
    "                                  P3D_NOSE,\n",
    "                                  P3D_SUB_NOSE,\n",
    "                                  P3D_RIGHT_EYE,\n",
    "                                  P3D_RIGHT_TEAR,\n",
    "                                  P3D_LEFT_TEAR,\n",
    "                                  P3D_LEFT_EYE,\n",
    "                                  P3D_STOMION])\n",
    "\n",
    "    #Declaring the two classifiers\n",
    "    #my_cascade = haarCascade(\"../etc/haarcascade_frontalface_alt.xml\", \"../etc/haarcascade_profileface.xml\")\n",
    "    dlib_landmarks_file = \"./shape_predictor_68_face_landmarks.dat\"\n",
    "    if(os.path.isfile(dlib_landmarks_file)==False): \n",
    "        print(\"The dlib landmarks file is missing! Use the following commands to download and unzip: \")\n",
    "        print(\">> wget dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "        print(\">> bzip2 -d shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "        return\n",
    "    my_detector = faceLandmarkDetection(dlib_landmarks_file)\n",
    "    my_face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    while(True):\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = video_capture.read()\n",
    "        #gray = cv2.cvtColor(frame[roi_y1:roi_y2, roi_x1:roi_x2], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces_array = my_face_detector(frame, 1)\n",
    "        print(\"Total Faces: \" + str(len(faces_array)))\n",
    "        for i, pos in enumerate(faces_array):\n",
    "\n",
    "            face_x1 = pos.left()\n",
    "            face_y1 = pos.top()\n",
    "            face_x2 = pos.right()\n",
    "            face_y2 = pos.bottom()\n",
    "            text_x1 = face_x1\n",
    "            text_y1 = face_y1 - 3\n",
    "\n",
    "            cv2.putText(frame, \"FACE \" + str(i+1), (text_x1,text_y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1);\n",
    "            cv2.rectangle(frame, \n",
    "                         (face_x1, face_y1), \n",
    "                         (face_x2, face_y2), \n",
    "                         (0, 255, 0), \n",
    "                          2)            \n",
    "\n",
    "            landmarks_2D = my_detector.returnLandmarks(frame, face_x1, face_y1, face_x2, face_y2, points_to_return=TRACKED_POINTS)\n",
    "\n",
    "\n",
    "            for point in landmarks_2D:\n",
    "                cv2.circle(frame,( point[0], point[1] ), 2, (0,0,255), -1)\n",
    "\n",
    "\n",
    "            #Applying the PnP solver to find the 3D pose\n",
    "            # of the head from the 2D position of the\n",
    "            # landmarks.\n",
    "            #retval - bool\n",
    "            #rvec - Output rotation vector that, together with tvec, brings \n",
    "            # points from the model coordinate system to the camera coordinate system.\n",
    "            #tvec - Output translation vector.\n",
    "            retval, rvec, tvec = cv2.solvePnP(landmarks_3D, \n",
    "                                                  landmarks_2D, \n",
    "                                                  camera_matrix, camera_distortion)\n",
    "\n",
    "            #Now we project the 3D points into the image plane\n",
    "            #Creating a 3-axis to be used as reference in the image.\n",
    "            axis = numpy.float32([[50,0,0], \n",
    "                                      [0,50,0], \n",
    "                                      [0,0,50]])\n",
    "            imgpts, jac = cv2.projectPoints(axis, rvec, tvec, camera_matrix, camera_distortion)\n",
    "\n",
    "            #Drawing the three axis on the image frame.\n",
    "            #The opencv colors are defined as BGR colors such as: \n",
    "            # (a, b, c) >> Blue = a, Green = b and Red = c\n",
    "            #Our axis/color convention is X=R, Y=G, Z=B\n",
    "            sellion_xy = (landmarks_2D[7][0], landmarks_2D[7][1])\n",
    "            cv2.line(frame, sellion_xy, tuple(imgpts[1].ravel()), (0,255,0), 3) #GREEN\n",
    "            cv2.line(frame, sellion_xy, tuple(imgpts[2].ravel()), (255,0,0), 3) #BLUE\n",
    "            cv2.line(frame, sellion_xy, tuple(imgpts[0].ravel()), (0,0,255), 3) #RED\n",
    "\n",
    "        #Writing in the output file\n",
    "        out.write(frame)\n",
    "\n",
    "        #Showing the frame and waiting\n",
    "        # for the exit command\n",
    "        cv2.imshow('Video', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "  \n",
    "    #Release the camera\n",
    "    video_capture.release()\n",
    "    print(\"Bye...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
